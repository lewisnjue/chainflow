{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f89a9636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from nnetflow import tensor\n",
    "from sklearn.datasets import make_regression \n",
    "from sklearn.model_selection import train_test_split \n",
    "from nnetflow import layers, optim, losses , module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bee58336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (800, 10)\n",
      "y_train shape: (800,)\n",
      "y_train reshaped: (800, 1)\n"
     ]
    }
   ],
   "source": [
    "X,y  = make_regression(n_samples=1000, n_features=10, noise=0.1) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "y_train = y_train.reshape(-1, 1) \n",
    "print(\"y_train reshaped:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "075a4d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tensor(X_train, dtype=\"float32\",require_grad=False)\n",
    "y_train = tensor(y_train, dtype=\"float32\",require_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d001682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 10), (800, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee7f81ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(module.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = layers.Linear(10, 64)  # shape -> (800,64) \n",
    "        self.fc2 = layers.Linear(64, 32)# shape -> (800,32)\n",
    "        self.fc3 = layers.Linear(32, 1) # output a tensor of shappe -> (800,1)\n",
    "    def forward(self, x): \n",
    "        # x shape -> (800,10)\n",
    "        x = self.fc1(x) # shape -> (800,64)\n",
    "        x = self.fc2(x) # shape -> (800,32)\n",
    "        x = self.fc3(x) # shape -> (800,1)\n",
    "        return x\n",
    "\n",
    "# le me track my bacward pass logic and shapes \n",
    "# output loss is a scaler -> shape (1,)\n",
    "# the first one will be dL/dx -> shape (800,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93d86402",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3e65053",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "loss_fn = losses.MSELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769488c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe97d240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 800 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m loss = loss_fn(y_pred, y_train) \u001b[38;5;66;03m# reshape to a scaler \u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(loss.shape)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m      8\u001b[39m optimizer.step() \n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (epoch + \u001b[32m1\u001b[39m) % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m: \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/nnetflow/nnetflow/engine.py:142\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, grad_clip)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# Backprop\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(topo):\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# Safe-guard\u001b[39;00m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m v.device == \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/nnetflow/nnetflow/engine.py:438\u001b[39m, in \u001b[36mTensor.__matmul__.<locals>._backward\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m other.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    437\u001b[39m     other.grad = np.zeros_like(other.data) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device == \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m cp.zeros_like(other.data)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28mself\u001b[39m.grad += \u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\n\u001b[32m    439\u001b[39m other.grad += \u001b[38;5;28mself\u001b[39m.data.T @ grad\n",
      "\u001b[31mValueError\u001b[39m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 800 is different from 1)"
     ]
    }
   ],
   "source": [
    "# training loop \n",
    "for epoch in range(100): \n",
    "    optimizer.zero_grad() \n",
    "    y_pred = model(X_train) \n",
    "    loss = loss_fn(y_pred, y_train) # reshape to a scaler \n",
    "    print(loss.shape)\n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "\n",
    "    if (epoch + 1) % 10 == 0: \n",
    "        print(f\"Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a92855f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2653045",
   "metadata": {},
   "source": [
    "# Results and Next Steps\n",
    "\n",
    "The MLP model has been successfully trained using the custom nnetflow framework. The training loop completed 100 epochs, and the loss was printed every 10 epochs. You can now:\n",
    "\n",
    "- Evaluate the model on the test set.\n",
    "- Experiment with different architectures (e.g., add more layers, change activation functions).\n",
    "- Try other optimizers or loss functions.\n",
    "- Extend the framework with new features (e.g., dropout, batch normalization).\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Evaluate on Test Set**\n",
    "\n",
    "You can run the following code to evaluate the model's performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1343df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "X_test_tensor = tensor(X_test, dtype=\"float32\", require_grad=False)\n",
    "y_test_tensor = tensor(y_test, dtype=\"float32\", require_grad=False)\n",
    "\n",
    "# Evaluate\n",
    "model.eval = True  # If you add dropout/batchnorm later, this will be useful\n",
    "y_pred_test = model(X_test_tensor)\n",
    "test_loss = loss_fn(y_pred_test, y_test_tensor)\n",
    "print(f\"Test Loss: {test_loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
